# -*- coding: utf-8 -*-
"""Proyecto1-Regresion.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1SVXCqpyp9lsyHJqjv_GNsdAswfMGwDLn
"""

import numpy as np
from google.colab import drive
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from google.colab import files
import requests
#drive.mount('/content/drive')

url = "https://raw.githubusercontent.com/aljozu/ml_regresion/main/datos.txt"
response = requests.get(url)
data = response.text

data_list = [float(x.strip()) for x in data.split(',')]
x_values = np.arange(len(data_list))

print(data_list)
print(len(data_list))

sns.scatterplot(x=x_values, y=data_list)

plt.title("Scatter Plot of datos.txt")
plt.xlabel("Time step")
plt.ylabel("Values")
# Save the plot as an SVG file
#plt.savefig("scatter_plot.svg", format='svg')
#files.download("scatter_plot.svg")

plt.show()

# Plot the time series using Seaborn
sns.lineplot(x=x_values, y=data_list)

plt.title("Time Series Plot")
plt.xlabel("Time Steps")
plt.ylabel("Values")
plt.show()

"""# Check for stationarity"""

from statsmodels.tsa.stattools import adfuller

# Perform Augmented Dickey-Fuller (ADF) test to check for stationarity
adf_test = adfuller(data_list)

# Extract results
adf_statistic = adf_test[0]
p_value = adf_test[1]
critical_values = adf_test[4]

# Display results
print(f'ADF Statistic: {adf_statistic}')
print(f'p-value: {p_value}')
print('Critical Values:')
for key, value in critical_values.items():
    print(f'   {key}: {value}')

# Interpret the result
if p_value < 0.05:
    print("The time series is stationary (reject the null hypothesis).")
else:
    print("The time series is not stationary (fail to reject the null hypothesis).")

class PolynomialRegression:
    def __init__(self, degree, learning_rate, iterations, alpha=0.1):
        self.degree = degree
        self.learning_rate = learning_rate
        self.iterations = iterations
        self.alpha = alpha  # Regularization strength

    def transform(self, X):
        X_transform = np.ones((len(X), self.degree + 1))
        for j in range(1, self.degree + 1):
            X_transform[:, j] = X ** j
        return X_transform

    def normalize(self, X):
        X_mean = np.mean(X[:, 1:], axis=0)
        X_std = np.std(X[:, 1:], axis=0)
        X[:, 1:] = (X[:, 1:] - X_mean) / X_std
        return X

    def fit(self, X, y):
        self.X = X
        self.y = y
        self.m = self.X.shape[0]

        X_transform = self.transform(self.X)
        X_normalize = self.normalize(X_transform)

        XtX = np.dot(X_normalize.T, X_normalize)  # X^T * X
        Xty = np.dot(X_normalize.T, self.y)        # X^T * y

        self.W = np.linalg.solve(XtX, Xty)  # Solve for weights

    def fit_with_l1(self, X, y):
        self.X = X
        self.y = y
        self.m = self.X.shape[0]

        X_transform = self.transform(self.X)
        X_normalize = self.normalize(X_transform)

        # Initialize weights
        self.W = np.zeros(X_normalize.shape[1])

        # Coordinate Descent
        for _ in range(self.iterations):
            for j in range(len(self.W)):
                if j == 0:  # Bias term, no regularization
                    self.W[j] = np.sum(self.y * X_normalize[:, j]) / np.sum(X_normalize[:, j] ** 2)
                else:
                    rho_j = np.dot(X_normalize[:, j], self.y - np.dot(X_normalize, self.W) + self.W[j] * X_normalize[:, j])
                    self.W[j] = np.sign(rho_j) * max(0, abs(rho_j) - self.alpha) / np.sum(X_normalize[:, j] ** 2)

    def fit_with_l2(self, X, y):
        self.X = X
        self.y = y
        self.m = self.X.shape[0]

        X_transform = self.transform(self.X)
        X_normalize = self.normalize(X_transform)

        XtX = np.dot(X_normalize.T, X_normalize)  # X^T * X
        Xty = np.dot(X_normalize.T, self.y)        # X^T * y

        # Add L2 regularization term
        reg_term = self.alpha * np.eye(XtX.shape[0])
        reg_term[0, 0] = 0  # No regularization for the bias term

        self.W = np.linalg.solve(XtX + reg_term, Xty)  # Solve for weights with L2 regularization

    def predict(self, X):
        X_transform = self.transform(X)
        X_normalize = self.normalize(X_transform)
        return np.dot(X_normalize, self.W)

# Implementaci贸n sin bibliotecas externas para calcular el MSE
def mean_squared_error_manual(y_true, y_pred):
    return np.mean((y_true - y_pred) ** 2)

Y = np.array([float(x.strip()) for x in data.split(',')])
X = np.arange(len(Y))

model_no_reg = PolynomialRegression(degree=100, learning_rate=0.01, iterations=1000, alpha=0.1)
model_ridge = PolynomialRegression(degree=100, learning_rate=0.01, iterations=1000, alpha=0.1)
model_lasso = PolynomialRegression(degree=100, learning_rate=0.01, iterations=1000, alpha=0.1)

model_no_reg.fit(X, Y)
model_ridge.fit_with_l1(X, Y)
model_lasso.fit_with_l2(X, Y)
# Prediction on training set
Y_no_reg = model_no_reg.predict(X)
Y_ridge = model_ridge.predict(X)
Y_lasso = model_lasso.predict(X)

# Scatter plot of actual data
sns.scatterplot(x=X, y=Y, color='blue', label='Actual')

# Line plots for different regression results
sns.lineplot(x=X, y=Y_no_reg, color='orange', label='No Regularization')
sns.lineplot(x=X, y=Y_ridge, color='green', label='L1 Regularization (Ridge)')
sns.lineplot(x=X, y=Y_lasso, color='purple', label='L2 Regularization (Lasso)')

plt.title('Polynomial Regression')
plt.xlabel('X')
plt.ylabel('Y')
plt.legend()
plt.show()

"""#5. Calculo del MSE"""

# Calcular el MSE para cada modelo
mse_no_reg = mean_squared_error_manual(Y, Y_no_reg)
mse_l1 = mean_squared_error_manual(Y, Y_ridge)
mse_l2 = mean_squared_error_manual(Y, Y_lasso)

print(f"MSE Sin Regularizaci贸n: {mse_no_reg}")
print(f"MSE Regularizaci贸n L1 (Lasso): {mse_l1}")
print(f"MSE Regularizaci贸n L2 (Ridge): {mse_l2}")